Node group autoscaling
======================
======================

To enable autoscaling for EKS node groups, you need two layers of configuration:

‚úÖ 1. EKS Managed Node Group Autoscaling (Terraform)

In your existing Terraform config (in main.tf), you're already defining scaling_config. 
Here's how to ensure it's correctly configured:

üîß Example

resource "aws_eks_node_group" "node_group" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.cluster_name}-ng"
  node_role_arn   = aws_iam_role.node_group_role.arn
  subnet_ids      = aws_subnet.private[*].id

  scaling_config {
    desired_size = 2
    max_size     = 5
    min_size     = 1
  }

  instance_types = ["t3.medium"]
  capacity_type  = "ON_DEMAND"  # or "SPOT"
}

‚úÖ 2. Cluster Autoscaler (Kubernetes Level)

EKS doesn‚Äôt autoscale nodes by itself. To react to pod demand, you need to install the Cluster Autoscaler in your cluster.

üîß Step-by-Step: Install Cluster Autoscaler (Helm Recommended)

helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm repo update

helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set cloudProvider=aws \
  --set autoDiscovery.clusterName=my-eks-cluster \
  --set awsRegion=us-east-1 \
  --set rbac.create=true \
  --set extraArgs.balance-similar-node-groups=true \
  --set extraArgs.expander=least-waste \
  --set extraArgs.skip-nodes-with-local-storage=false \
  --set extraArgs.skip-nodes-with-system-pods=false

Or use a YAML manifest if you don‚Äôt want Helm.


f you prefer not to use Helm to install the Cluster Autoscaler on your EKS cluster, you can apply a YAML manifest directly. Below is a basic example of the manifest you can modify and apply using kubectl apply -f.
‚úÖ Cluster Autoscaler Deployment YAML for AWS EKS (without Helm)

    Replace placeholders like my-eks-cluster, us-east-1, and the EKS node group tags accordingly.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints", "pods", "services", "nodes", "namespaces", "persistentvolumeclaims", "persistentvolumes"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["apps"]
    resources: ["replicasets", "statefulsets", "daemonsets", "deployments"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["autoscaling.k8s.io"]
    resources: ["*"]
    verbs: ["*"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.29.0
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --cloud-provider=aws
            - --cluster-name=my-eks-cluster
            - --scan-interval=10s
            - --balance-similar-node-groups
            - --skip-nodes-with-local-storage=false
            - --skip-nodes-with-system-pods=false
            - --expander=least-waste
            - --aws-use-static-instance-list=false
            - --nodes=1:10:your-node-group-name
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          env:
            - name: AWS_REGION
              value: us-east-1
      volumes:
        - name: ssl-certs
          hostPath:
            path: /etc/ssl/certs/ca-bundle.crt

üìå Notes:

    Replace my-eks-cluster with your actual EKS cluster name.

    Replace your-node-group-name with the EKS managed node group name tagged with k8s.io/cluster-autoscaler/enabled = true and k8s.io/cluster-autoscaler/<cluster-name> = owned.

    Update the image tag (v1.29.0) as needed to match your Kubernetes version.

    Make sure your IAM Role (IRSA or node role) has permissions: autoscaling:DescribeAutoScalingGroups, autoscaling:SetDesiredCapacity, etc.

Let me know if you need the IRSA setup (IAM role and trust policy) as well.


üìå IAM for Cluster Autoscaler (IRSA Recommended)

Recommended: use IAM Roles for Service Accounts (IRSA) for autoscaler pods.

If not using IRSA, attach this policy to your node group role:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeAutoScalingInstances",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeTags",
        "autoscaling:SetDesiredCapacity",
        "autoscaling:TerminateInstanceInAutoScalingGroup"
      ],
      "Effect": "Allow",
      "Resource": "*"
    }
  ]
}

‚úÖ Summary
Component	What It Does
scaling_config	Sets min/max/desired capacity in Terraform
Cluster Autoscaler	Adds/removes nodes dynamically at runtime
IAM permissions	Grants Cluster Autoscaler access to manage ASGs
Optional: IRSA	Secure service account access to AWS APIs

To include the Cluster Autoscaler installation (Helm or YAML) and IAM policy attachment into your Terraform repo setup
======================================================================================================================

Below is the complete integration of Cluster Autoscaler into your Terraform EKS setup using:

    ‚úÖ IAM Role for Service Account (IRSA)

    ‚úÖ Helm-based deployment of Cluster Autoscaler

    ‚úÖ Terraform-managed IAM policy

    ‚úÖ Optional: Terraform Helm provider for autoscaler install

üìÅ Additions to GitHub Repo Structure

Add this to your repo under a new folder:

eks-cluster-terraform/
‚îî‚îÄ‚îÄ terraform/
    ‚îú‚îÄ‚îÄ cluster-autoscaler/
    ‚îÇ   ‚îú‚îÄ‚îÄ iam-policy.json

üîê cluster-autoscaler/iam-policy.json


{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeAutoScalingInstances",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeTags",
        "autoscaling:SetDesiredCapacity",
        "autoscaling:TerminateInstanceInAutoScalingGroup"
      ],
      "Effect": "Allow",
      "Resource": "*"
    }
  ]
}


‚úçÔ∏è Add to main.tf (IRSA + Helm)

1. IAM Policy & Role for Autoscaler

resource "aws_iam_policy" "cluster_autoscaler" {
  name        = "EKSClusterAutoscalerPolicy"
  policy      = file("${path.module}/cluster-autoscaler/iam-policy.json")
}

resource "aws_iam_role" "cluster_autoscaler" {
  name = "${var.cluster_name}-autoscaler-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          Federated = aws_iam_openid_connect_provider.eks.arn
        }
        Action = "sts:AssumeRoleWithWebIdentity"
        Condition = {
          StringEquals = {
            "${replace(aws_iam_openid_connect_provider.eks.url, "https://", "")}:sub" = "system:serviceaccount:kube-system:cluster-autoscaler"
          }
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "cluster_autoscaler_attach" {
  role       = aws_iam_role.cluster_autoscaler.name
  policy_arn = aws_iam_policy.cluster_autoscaler.arn
}

2. OIDC Provider for IRSA

data "aws_eks_cluster" "cluster" {
  name = aws_eks_cluster.main.name
}

data "aws_eks_cluster_auth" "cluster" {
  name = aws_eks_cluster.main.name
}

resource "aws_iam_openid_connect_provider" "eks" {
  url             = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.oidc.certificates[0].sha1_fingerprint]
}

data "tls_certificate" "oidc" {
  url = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
}


üì¶ Optional: Install Cluster Autoscaler via Terraform Helm Provider

Add Helm provider block in provider.tf:

provider "helm" {
  kubernetes {
    host                   = data.aws_eks_cluster.cluster.endpoint
    cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
    token                  = data.aws_eks_cluster_auth.cluster.token
  }
}


Then add the Helm release:

resource "helm_release" "cluster_autoscaler" {
  name       = "cluster-autoscaler"
  repository = "https://kubernetes.github.io/autoscaler"
  chart      = "cluster-autoscaler"
  namespace  = "kube-system"
  version    = "9.29.0"

  set {
    name  = "autoDiscovery.clusterName"
    value = var.cluster_name
  }

  set {
    name  = "awsRegion"
    value = var.region
  }

  set {
    name  = "rbac.serviceAccount.create"
    value = "true"
  }

  set {
    name  = "rbac.serviceAccount.name"
    value = "cluster-autoscaler"
  }

  set {
    name  = "extraArgs.balance-similar-node-groups"
    value = "true"
  }

  set {
    name  = "extraArgs.expander"
    value = "least-waste"
  }

  set {
    name  = "extraArgs.skip-nodes-with-local-storage"
    value = "false"
  }

  set {
    name  = "extraArgs.skip-nodes-with-system-pods"
    value = "false"
  }

  set {
    name  = "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"
    value = aws_iam_role.cluster_autoscaler.arn
  }

  depends_on = [aws_iam_role_policy_attachment.cluster_autoscaler_attach]
}

‚úÖ What You Get
Feature	Value
EKS Node Group Autoscaling	Scales nodes between min/max automatically
Cluster Autoscaler	Reacts to pod demand to increase/decrease node count
IAM for Autoscaler	Uses secure IRSA role with minimal permissions
Helm Deployed	Automatically installs via Terraform + Helm provider




